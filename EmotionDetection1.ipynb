{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f5308d",
   "metadata": {},
   "source": [
    "# Emotion Detection Model\n",
    "This notebook demonstrates the process of building and evaluating an emotion detection model using machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb82e5",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a40f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import neattext.functions as nfx\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "import re \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08c713",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21175b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training labels: [0 3 2 5 4 1]\n",
      "Unique validation labels: [0 2 3 1 4 5]\n",
      "Unique test labels: [0 1 4 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_val = pd.read_csv(\"data/validation.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Check unique Emotions in each dataset\n",
    "print(\"Unique training labels:\", df_train['label'].unique())\n",
    "print(\"Unique validation labels:\", df_val['label'].unique())\n",
    "print(\"Unique test labels:\", df_test['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aa266",
   "metadata": {},
   "source": [
    "## Mapping Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66389f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df['emotion'] = df['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a1b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for Missing values in training data\n",
    "df_train.isnull().sum()\n",
    "\n",
    "#Value counts of each Emotions\n",
    "print(df_train['emotion'].value_counts())\n",
    "\n",
    "dir(nfx)\n",
    "df_train['Clean_Text'] = df_train['text'].apply(nfx.remove_userhandles)\n",
    "df_train['Clean_Text'] = df_train['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df_test['Clean_Text'] = df_test['text'].apply(nfx.remove_userhandles)\n",
    "df_test['Clean_Text'] = df_test['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df_val['Clean_Text'] = df_val['text'].apply(nfx.remove_userhandles)\n",
    "df_val['Clean_Text'] = df_val['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "# def clean_text(text):\n",
    "#     # Remove special characters and punctuation\n",
    "#     text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "#     # Convert to lowercase\n",
    "#     text = text.lower()\n",
    "# # Apply data cleaning to the 'text' column in each dataset\n",
    "# df_train['Clean_Text'] = df_train['Clean_Text'].apply(clean_text)\n",
    "# df_val['Clean_Text'] = df_val['Clean_Text'].apply(clean_text)\n",
    "# df_test['Clean_Text'] = df_test['Clean_Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f3cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(df_train['emotion'], bins=30, color='skyblue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fce924",
   "metadata": {},
   "source": [
    "## Text Preprocessing with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61b07737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Define a function for tokenization and stemming\n",
    "# def stemmed_tokenizer(text):\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get the Part of Speech (POS) for accurate lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if POS not found\n",
    "\n",
    "# Define a function for tokenization and lemmatization\n",
    "def lemmatized_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016330d7",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d61cac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the training data\n",
    "# vectorizer = CountVectorizer(tokenizer=lemmatized_tokenizer)\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatized_tokenizer, max_features=5000)\n",
    "\n",
    "# tdm_train = vectorizer.fit_transform(df_train['text'])\n",
    "tdm_train = vectorizer.fit_transform(df_train['Clean_Text'])\n",
    "tdm_val = vectorizer.transform(df_val['Clean_Text'])\n",
    "tdm_test = vectorizer.transform(df_test['Clean_Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730c780",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a1a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (OvR) Cross-Validation Accuracy: 0.8477499999999999\n",
      "Logistic Regression (OvR) Validation Accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "# # Train a Logistic Regression model\n",
    "# log_model = LogisticRegression(max_iter=1000)\n",
    "# log_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# # Validate the model\n",
    "# # tdm_val = vectorizer.transform(df_val['text'])\n",
    "# tdm_val = vectorizer.transform(df_val['Clean_Text'])\n",
    "# y_val_pred_log = log_model.predict(tdm_val)\n",
    "\n",
    "# # Calculate and print validation accuracy\n",
    "# val_accuracy_log = accuracy_score(df_val['label'], y_val_pred_log)\n",
    "# print(\"Logistic Regression Validation Accuracy:\", val_accuracy_log)\n",
    "\n",
    "log_model_ovr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "log_model_ovr.fit(tdm_train, df_train['label'])\n",
    "log_cv_scores = cross_val_score(log_model_ovr, tdm_train, df_train['label'], cv=5, scoring='accuracy')\n",
    "print(\"Logistic Regression (OvR) Cross-Validation Accuracy:\", log_cv_scores.mean())\n",
    "\n",
    "# Validate the model\n",
    "tdm_val = vectorizer.transform(df_val['Clean_Text'])\n",
    "y_val_pred_log_ovr = log_model_ovr.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_log_ovr = accuracy_score(df_val['label'], y_val_pred_log_ovr)\n",
    "print(\"Logistic Regression (OvR) Validation Accuracy:\", val_accuracy_log_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33fec8",
   "metadata": {},
   "source": [
    "## Training the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d1cd455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Accuracy: 0.876\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Random Forest model\n",
    "y_val_pred_rf = rf_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_rf = accuracy_score(df_val['label'], y_val_pred_rf)\n",
    "print(\"Random Forest Validation Accuracy:\", val_accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767b2c",
   "metadata": {},
   "source": [
    "## Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "790b3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated LinearSVC Validation Accuracy: 0.8855\n"
     ]
    }
   ],
   "source": [
    "# #Train the SVM model\n",
    "# svm_model = SVC(probability=True,random_state=42)\n",
    "# svm_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# # Validate the SVM model\n",
    "# y_val_pred_svm = svm_model.predict(tdm_val)\n",
    "\n",
    "# # Calculate and print validation accuracy\n",
    "# val_accuracy_svm = accuracy_score(df_val['label'], y_val_pred_svm)\n",
    "# print(\"SVM Validation Accuracy:\", val_accuracy_svm)\n",
    "\n",
    "base_svm_model = LinearSVC(random_state=42)\n",
    "svm_model = CalibratedClassifierCV(base_svm_model)  # This enables probability estimates\n",
    "svm_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the calibrated model\n",
    "y_val_pred_svm = svm_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_svm = accuracy_score(df_val['label'], y_val_pred_svm)\n",
    "print(\"Calibrated LinearSVC Validation Accuracy:\", val_accuracy_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd732e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Validation Accuracy: 0.752\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Naive Bayes model\n",
    "y_val_pred_nb = nb_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_nb = accuracy_score(df_val['label'], y_val_pred_nb)\n",
    "print(\"Naive Bayes Validation Accuracy:\", val_accuracy_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00b2f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Accuracy: 0.8795\n"
     ]
    }
   ],
   "source": [
    "xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the XGBoost model\n",
    "y_val_pred_xgb = xgb_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_xgb = accuracy_score(df_val['label'], y_val_pred_xgb)\n",
    "print(\"XGBoost Validation Accuracy:\", val_accuracy_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d91ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "# Train a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Decision Tree model\n",
    "y_val_pred_dt = dt_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_dt = accuracy_score(df_val['label'], y_val_pred_dt)\n",
    "print(\"Decision Tree Validation Accuracy:\", val_accuracy_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05f9752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.8746875 0.8796875 0.8853125 0.87625   0.8825   ]\n",
      "Mean CV Accuracy: 0.8796875\n",
      "Combined Model Validation Accuracy: 0.884\n",
      "Combined Model Test Accuracy: 0.8885\n"
     ]
    }
   ],
   "source": [
    "combined_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('log_reg', log_model_ovr),\n",
    "        ('rf', rf_model),\n",
    "        ('svm', svm_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('nb', nb_model),\n",
    "    ],\n",
    "    voting='soft' \n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(combined_model, tdm_train, df_train['label'], cv=skf, scoring='accuracy')\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", np.mean(cv_scores))\n",
    "\n",
    "# Train the Combined Model\n",
    "combined_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validation\n",
    "y_val_pred_combined = combined_model.predict(tdm_val)\n",
    "val_accuracy_combined = accuracy_score(df_val['label'], y_val_pred_combined)\n",
    "print(\"Combined Model Validation Accuracy:\", val_accuracy_combined)\n",
    "\n",
    "# Testing the Combined Model\n",
    "y_test_pred_combined = combined_model.predict(tdm_test)\n",
    "test_accuracy_combined = accuracy_score(df_test['label'], y_test_pred_combined)\n",
    "print(\"Combined Model Test Accuracy:\", test_accuracy_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8d0a2",
   "metadata": {},
   "source": [
    "## Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107efea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the best model based on validation accuracy\n",
    "# if val_accuracy_log > val_accuracy_rf and val_accuracy_log > val_accuracy_svm:\n",
    "#     best_model = log_model\n",
    "#     best_model_name = \"Logistic Regression\"\n",
    "#     best_accuracy = val_accuracy_log\n",
    "# elif val_accuracy_rf > val_accuracy_log and val_accuracy_rf > val_accuracy_svm:\n",
    "#     best_model = rf_model\n",
    "#     best_model_name = \"Random Forest\"\n",
    "#     best_accuracy = val_accuracy_rf\n",
    "# else:\n",
    "#     best_model = svm_model\n",
    "#     best_model_name = \"SVM\"\n",
    "#     best_accuracy = val_accuracy_svm\n",
    "\n",
    "# print(f\"The best model is {best_model_name} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c6f0198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is SVM with accuracy: 0.8855\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model based on validation accuracy\n",
    "models = {\n",
    "#     \"Logistic Regression\": (log_model, val_accuracy_log),\n",
    "    \"Logistic Regression\": (log_model_ovr, val_accuracy_log_ovr),\n",
    "    \"Random Forest\": (rf_model, val_accuracy_rf),\n",
    "    \"SVM\": (svm_model, val_accuracy_svm),\n",
    "    \"Naive Bayes\": (nb_model, val_accuracy_nb),\n",
    "    \"XGBoost\": (xgb_model, val_accuracy_xgb),\n",
    "}\n",
    "\n",
    "# Find the best model\n",
    "best_model_name, (best_model, best_accuracy) = max(models.items(), key=lambda item: item[1][1])\n",
    "\n",
    "print(f\"The best model is {best_model_name} with accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d3c25",
   "metadata": {},
   "source": [
    "## Testing the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a57824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the best model: 0.8765\n"
     ]
    }
   ],
   "source": [
    "# Prepare the test data and make predictions\n",
    "tdm_test = vectorizer.transform(df_test['text'])\n",
    "y_test_pred = best_model.predict(tdm_test)\n",
    "\n",
    "# Calculate and print test accuracy\n",
    "test_accuracy = accuracy_score(df_test['label'], y_test_pred)\n",
    "print(\"Test Accuracy of the best model:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b5021",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "364b4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for the Test Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       581\n",
      "           1       0.90      0.93      0.91       695\n",
      "           2       0.81      0.71      0.76       159\n",
      "           3       0.83      0.85      0.84       275\n",
      "           4       0.88      0.81      0.84       224\n",
      "           5       0.75      0.70      0.72        66\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.85      0.82      0.83      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n",
      "\n",
      "Confusion Matrix for the Test Dataset:\n",
      "[[531  19   1  23   5   2]\n",
      " [  9 646  24   6   4   6]\n",
      " [  8  32 113   4   1   1]\n",
      " [ 22  10   1 235   6   1]\n",
      " [ 21   4   0  12 182   5]\n",
      " [  2   7   0   2   9  46]]\n"
     ]
    }
   ],
   "source": [
    "# Print classification report and confusion matrix for the test dataset\n",
    "print(\"\\nClassification Report for the Test Dataset:\")\n",
    "print(classification_report(df_test['label'], y_test_pred))\n",
    "print(\"\\nConfusion Matrix for the Test Dataset:\")\n",
    "print(confusion_matrix(df_test['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ee54c",
   "metadata": {},
   "source": [
    "## Adding Predictions to the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68d6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted emotions to the test dataframe\n",
    "df_test['predicted_emotion'] = [\n",
    "    label_mapping[label] if label in label_mapping else 'unknown'\n",
    "    for label in y_test_pred\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466d394",
   "metadata": {},
   "source": [
    "## Displaying Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7560c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Dataset Predictions:\n",
      "                                                text  emotion  \\\n",
      "0  im feeling rather rotten so im not very ambiti...  sadness   \n",
      "1          im updating my blog because i feel shitty  sadness   \n",
      "2  i never make her separate from me because i do...  sadness   \n",
      "3  i left with my bouquet of red and yellow tulip...      joy   \n",
      "4    i was feeling a little vain when i did this one  sadness   \n",
      "\n",
      "  predicted_emotion  \n",
      "0           sadness  \n",
      "1           sadness  \n",
      "2           sadness  \n",
      "3               joy  \n",
      "4           sadness  \n"
     ]
    }
   ],
   "source": [
    "# Display the predictions in the test dataset\n",
    "print(\"\\nTest Dataset Predictions:\")\n",
    "print(df_test[['text', 'emotion', 'predicted_emotion']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b00f3",
   "metadata": {},
   "source": [
    "## Predicting Emotion Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b57913b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(sentence, model, vectorizer, label_mapping):\n",
    "    sentence_tdm = vectorizer.transform([sentence])\n",
    "    probabilities = model.predict_proba(sentence_tdm)[0]\n",
    "    emotions = [label_mapping[i] for i in range(len(probabilities))]\n",
    "    \n",
    "    # Plot the predicted probabilities\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=emotions, y=probabilities)\n",
    "    plt.title('Emotion Prediction')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c899a5",
   "metadata": {},
   "source": [
    "## Using the Predict Emotion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe36ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predict emotion for a specific sentence\n",
    "# predict_emotion(\"I heard strange noises outside last night, and I couldn't sleep at all. My heart was racing, and I kept imagining all sorts of dangers lurking in the dark.\", best_model, vectorizer, label_mapping)\n",
    "\n",
    "# Function for user input prediction\n",
    "def user_input_prediction(model, vectorizer, label_mapping):\n",
    "    try:\n",
    "        sentence = input(\"Please enter a sentence to predict emotion: \")\n",
    "        predict_emotion(sentence, model, vectorizer, label_mapping)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the user input function\n",
    "user_input_prediction(best_model, vectorizer, label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fedce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
