{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f5308d",
   "metadata": {},
   "source": [
    "# Emotion Detection Model\n",
    "This notebook demonstrates the process of building and evaluating an emotion detection model using machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb82e5",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a40f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import neattext.functions as nfx\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    jaccard_score,\n",
    "    hamming_loss,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08c713",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21175b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training labels: [0 3 2 5 4 1]\n",
      "Unique validation labels: [0 2 3 1 4 5]\n",
      "Unique test labels: [0 1 4 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_val = pd.read_csv(\"data/validation.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Check unique Emotions in each dataset\n",
    "print(\"Unique training labels:\", df_train['label'].unique())\n",
    "print(\"Unique validation labels:\", df_val['label'].unique())\n",
    "print(\"Unique test labels:\", df_test['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aa266",
   "metadata": {},
   "source": [
    "## Mapping Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66389f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df['emotion'] = df['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505c6e3",
   "metadata": {},
   "source": [
    "## Data Cleaning Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a1b7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training set: text       0\n",
      "label      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "\n",
      "Training set emotion distribution:\n",
      " emotion\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for Missing values in training data\n",
    "print(\"Missing values in training set:\", df_train.isnull().sum())\n",
    "\n",
    "#Value counts of each Emotions\n",
    "print(\"\\nTraining set emotion distribution:\\n\", df_train['emotion'].value_counts())\n",
    "\n",
    "dir(nfx)\n",
    "df_train['Clean_Text'] = df_train['text'].apply(nfx.remove_userhandles)\n",
    "df_train['Clean_Text'] = df_train['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df_test['Clean_Text'] = df_test['text'].apply(nfx.remove_userhandles)\n",
    "df_test['Clean_Text'] = df_test['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df_val['Clean_Text'] = df_val['text'].apply(nfx.remove_userhandles)\n",
    "df_val['Clean_Text'] = df_val['Clean_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fce924",
   "metadata": {},
   "source": [
    "## Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b07737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Define a function for tokenization and stemming\n",
    "# def stemmed_tokenizer(text):\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get the Part of Speech (POS) for accurate lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if POS not found\n",
    "\n",
    "# Define a function for tokenization and lemmatization\n",
    "def lemmatized_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016330d7",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d61cac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the training data\n",
    "# vectorizer = CountVectorizer(tokenizer=lemmatized_tokenizer)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatized_tokenizer, max_features=5000)\n",
    "\n",
    "tdm_train = vectorizer.fit_transform(df_train['Clean_Text'])\n",
    "tdm_val = vectorizer.transform(df_val['Clean_Text'])\n",
    "tdm_test = vectorizer.transform(df_test['Clean_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957a4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Models\n",
    "X_train = tdm_train.toarray()\n",
    "y_train = df_train['label'].values\n",
    "X_val = tdm_val.toarray()\n",
    "y_val = df_val['label'].values\n",
    "\n",
    "# Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(tdm_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd8223",
   "metadata": {},
   "source": [
    "## Model Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56a79b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, y_val):\n",
    "    y_pred = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    jaccard = jaccard_score(y_val, y_pred, average='weighted')\n",
    "    hamming = hamming_loss(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"Jaccard Index: {jaccard:.4f}\")\n",
    "    print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred, target_names=label_mapping.values()))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730c780",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model - OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a1a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (OvR) Validation Accuracy: 0.8645\n",
      "Test Accuracy: 0.8405\n",
      "Jaccard Index: 0.7364\n",
      "Hamming Loss: 0.1595\n",
      "Precision: 0.8644\n",
      "Recall: 0.8405\n",
      "F1 Score: 0.8459\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.92      0.82      0.87       581\n",
      "         joy       0.95      0.82      0.88       695\n",
      "        love       0.62      0.89      0.73       159\n",
      "       anger       0.80      0.90      0.85       275\n",
      "        fear       0.81      0.82      0.81       224\n",
      "    surprise       0.52      0.91      0.66        66\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.77      0.86      0.80      2000\n",
      "weighted avg       0.86      0.84      0.85      2000\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[477  21  17  36  21   9]\n",
      " [ 12 571  66  13  13  20]\n",
      " [  7   4 142   2   1   3]\n",
      " [ 10   4   3 247   7   4]\n",
      " [  9   1   1  10 184  19]\n",
      " [  1   2   0   1   2  60]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "ovr_model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "ovr_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred = ovr_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_ovr = accuracy_score(df_val['label'], y_val_pred)\n",
    "print(\"Logistic Regression (OvR) Validation Accuracy:\", val_accuracy_ovr)\n",
    "evaluate_model(ovr_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b4000e",
   "metadata": {},
   "source": [
    "## Logistic Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00e1fdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000, regularization_strength=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Avoid overflow for large/small z\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_loss(self, y, y_predicted):\n",
    "        # Binary cross-entropy loss with L2 regularization\n",
    "        num_samples = y.shape[0]\n",
    "        loss = (-1 / num_samples) * np.sum(\n",
    "            y * np.log(y_predicted + 1e-15) + (1 - y) * np.log(1 - y_predicted + 1e-15)\n",
    "        )\n",
    "        reg_loss = (self.regularization_strength / (2 * num_samples)) * np.sum(self.weights ** 2)\n",
    "        return loss + reg_loss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.random.randn(num_features) * 0.01 \n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y)) + \\\n",
    "                 (self.regularization_strength / num_samples) * self.weights\n",
    "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return np.array([1 if i > 0.5 else 0 for i in y_predicted])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4401f1",
   "metadata": {},
   "source": [
    "## OneVsRest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e63e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVsRestClassifierScratch:\n",
    "    def __init__(self, base_classifier):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.models = []\n",
    "\n",
    "        for i in range(num_classes):\n",
    "            binary_y = np.where(y == i, 1, 0)\n",
    "            model = LogisticRegressionScratch(\n",
    "                learning_rate=self.base_classifier.learning_rate,\n",
    "                num_iterations=self.base_classifier.num_iterations,\n",
    "                regularization_strength=self.base_classifier.regularization_strength\n",
    "            )\n",
    "            model.fit(X, binary_y)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models]).T\n",
    "        return np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe890d9",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model - OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07e4b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the One-vs-Rest Logistic Regression model\n",
    "# ovr_model = OneVsRestClassifierScratch(\n",
    "#     LogisticRegressionScratch(learning_rate=0.01, num_iterations=1000, regularization_strength=0.1)\n",
    "# )\n",
    "# ovr_model.fit(X_train, y_train)\n",
    "\n",
    "# # Validate the model\n",
    "# y_val_pred = ovr_model.predict(X_val)\n",
    "\n",
    "# # Calculate and print validation accuracy\n",
    "# val_accuracy_ovr = accuracy_score(df_val['label'], y_val_pred)\n",
    "# print(\"One-vs-Rest Logistic Regression Validation Accuracy:\", val_accuracy_ovr)\n",
    "# evaluate_model(ovr_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33fec8",
   "metadata": {},
   "source": [
    "## Training the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Random Forest model\n",
    "y_val_pred_rf = rf_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_rf = accuracy_score(df_val['label'], y_val_pred_rf)\n",
    "print(\"Random Forest Validation Accuracy:\", val_accuracy_rf)\n",
    "evaluate_model(rf_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767b2c",
   "metadata": {},
   "source": [
    "## Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790b3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a SVM model\n",
    "base_svm_model = LinearSVC(random_state=42)\n",
    "svm_model = CalibratedClassifierCV(base_svm_model)\n",
    "svm_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the calibrated model\n",
    "y_val_pred_svm = svm_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_svm = accuracy_score(df_val['label'], y_val_pred_svm)\n",
    "print(\"Calibrated LinearSVC Validation Accuracy:\", val_accuracy_svm)\n",
    "evaluate_model(svm_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711025f2",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd732e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Naive Bayes model\n",
    "y_val_pred_nb = nb_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_nb = accuracy_score(df_val['label'], y_val_pred_nb)\n",
    "print(\"Naive Bayes Validation Accuracy:\", val_accuracy_nb)\n",
    "evaluate_model(nb_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79555730",
   "metadata": {},
   "source": [
    "## Training the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d91ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Decision Tree model\n",
    "y_val_pred_dt = dt_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_dt = accuracy_score(df_val['label'], y_val_pred_dt)\n",
    "print(\"Decision Tree Validation Accuracy:\", val_accuracy_dt)\n",
    "evaluate_model(dt_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9203f1",
   "metadata": {},
   "source": [
    "## Decision Tree Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f554c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeScratch:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def gini(self, y):\n",
    "        \"\"\"Calculate Gini Impurity.\"\"\"\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum(probabilities**2)\n",
    "\n",
    "    def split(self, X_column, threshold):\n",
    "        \"\"\"Split the dataset based on a feature threshold.\"\"\"\n",
    "        left_indices = np.where(X_column <= threshold)[0]\n",
    "        right_indices = np.where(X_column > threshold)[0]\n",
    "        return left_indices, right_indices\n",
    "\n",
    "    def information_gain(self, y, left_indices, right_indices):\n",
    "        \"\"\"Calculate Information Gain from a split.\"\"\"\n",
    "        weight_left = len(left_indices) / len(y)\n",
    "        weight_right = len(right_indices) / len(y)\n",
    "        gain = self.gini(y)\n",
    "        gain -= weight_left * self.gini(y[left_indices])\n",
    "        gain -= weight_right * self.gini(y[right_indices])\n",
    "        return gain\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"Find the best split for the dataset.\"\"\"\n",
    "        best_gain = -1\n",
    "        best_split = None\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_idx])\n",
    "            for threshold in thresholds:\n",
    "                left_indices, right_indices = self.split(X[:, feature_idx], threshold)\n",
    "                if len(left_indices) < self.min_samples_split or len(right_indices) < self.min_samples_split:\n",
    "                    continue\n",
    "\n",
    "                gain = self.information_gain(y, left_indices, right_indices)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        \"feature_index\": feature_idx,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"left_indices\": left_indices,\n",
    "                        \"right_indices\": right_indices,\n",
    "                    }\n",
    "        return best_split\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"Recursively build the tree.\"\"\"\n",
    "        if self.max_depth and depth >= self.max_depth:\n",
    "            return np.bincount(y).argmax()\n",
    "        if len(np.unique(y)) == 1 or len(y) < self.min_samples_split:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        split = self.best_split(X, y)\n",
    "        if not split:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        left_tree = self.build_tree(X[split[\"left_indices\"]], y[split[\"left_indices\"]], depth + 1)\n",
    "        right_tree = self.build_tree(X[split[\"right_indices\"]], y[split[\"right_indices\"]], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature_index\": split[\"feature_index\"],\n",
    "            \"threshold\": split[\"threshold\"],\n",
    "            \"left_tree\": left_tree,\n",
    "            \"right_tree\": right_tree,\n",
    "        }\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_single(self, x, tree):\n",
    "        \"\"\"Predict the class for a single sample.\"\"\"\n",
    "        if isinstance(tree, dict):\n",
    "            feature_index = tree[\"feature_index\"]\n",
    "            threshold = tree[\"threshold\"]\n",
    "            if x[feature_index] <= threshold:\n",
    "                return self.predict_single(x, tree[\"left_tree\"])\n",
    "            else:\n",
    "                return self.predict_single(x, tree[\"right_tree\"])\n",
    "        return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(x, self.tree) for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d4a54",
   "metadata": {},
   "source": [
    "## Training the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train custom Decision Tree model\n",
    "# dt_model = DecisionTreeScratch(max_depth=10, min_samples_split=10)\n",
    "# dt_model.fit(X_train, y_train)\n",
    "\n",
    "# # Validate the model\n",
    "# y_val_pred = dt_model.predict(X_val)\n",
    "\n",
    "# # Calculate validation accuracy\n",
    "# val_accuracy_custom_dt = accuracy_score(df_val['label'], y_val_pred)\n",
    "# print(f\"Custom Decision Tree Validation Accuracy: {val_accuracy_custom_dt:.4f}\")\n",
    "# evaluate_model(dt_model, X_test, df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e1ac7",
   "metadata": {},
   "source": [
    "## Comparing Models and Testing the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracies = {\n",
    "    \"Logistic Regression\": val_accuracy_ovr,\n",
    "    \"Random Forest\": val_accuracy_rf,\n",
    "    \"SVM\": val_accuracy_svm,\n",
    "    \"Naive Bayes\": val_accuracy_nb,\n",
    "    \"Decision Tree\": val_accuracy_dt,\n",
    "}\n",
    "\n",
    "best_model_name = max(model_accuracies, key=model_accuracies.get)\n",
    "print(f\"The best model is {best_model_name} with accuracy: {model_accuracies[best_model_name]}\")\n",
    "\n",
    "best_model = {\n",
    "    \"Logistic Regression\": ovr_model,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"SVM\": svm_model,\n",
    "    \"Naive Bayes\": nb_model,\n",
    "    \"Decision Tree\": dt_model,\n",
    "}[best_model_name]\n",
    "y_test_pred = best_model.predict(tdm_test)\n",
    "test_accuracy = accuracy_score(df_test['label'], y_test_pred)\n",
    "print(f\"Test Accuracy with {best_model_name}: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b5021",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report and confusion matrix for the test dataset\n",
    "print(\"\\nClassification Report for the Test Dataset:\")\n",
    "print(classification_report(df_test['label'], y_test_pred))\n",
    "print(\"\\nConfusion Matrix for the Test Dataset:\")\n",
    "print(confusion_matrix(df_test['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ee54c",
   "metadata": {},
   "source": [
    "## Adding Predictions to the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted emotions to the test dataframe\n",
    "df_test['predicted_emotion'] = [\n",
    "    label_mapping[label] if label in label_mapping else 'unknown'\n",
    "    for label in y_test_pred\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466d394",
   "metadata": {},
   "source": [
    "## Displaying Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions in the test dataset\n",
    "print(\"\\nTest Dataset Predictions:\")\n",
    "print(df_test[['text', 'emotion', 'predicted_emotion']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b00f3",
   "metadata": {},
   "source": [
    "## Predicting Emotion Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57913b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(sentence, model, vectorizer, label_mapping):\n",
    "    sentence_tdm = vectorizer.transform([sentence])\n",
    "    probabilities = model.predict_proba(sentence_tdm)[0]\n",
    "    emotions = [label_mapping[i] for i in range(len(probabilities))]\n",
    "    \n",
    "    # Plot the predicted probabilities\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=emotions, y=probabilities)\n",
    "    plt.title('Emotion Prediction')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c899a5",
   "metadata": {},
   "source": [
    "## Using the Predict Emotion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe36ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predict emotion for a specific sentence\n",
    "# predict_emotion(\"I heard strange noises outside last night, and I couldn't sleep at all. My heart was racing, and I kept imagining all sorts of dangers lurking in the dark.\", best_model, vectorizer, label_mapping)\n",
    "\n",
    "# Function for user input prediction\n",
    "def user_input_prediction(model, vectorizer, label_mapping):\n",
    "    try:\n",
    "        sentence = input(\"Please enter a sentence to predict emotion: \")\n",
    "        predict_emotion(sentence, model, vectorizer, label_mapping)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the user input function\n",
    "user_input_prediction(best_model, vectorizer, label_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
