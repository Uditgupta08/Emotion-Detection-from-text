{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f5308d",
   "metadata": {},
   "source": [
    "# Emotion Detection Model\n",
    "This notebook demonstrates the process of building and evaluating an emotion detection model using machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb82e5",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a40f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\udit0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import neattext.functions as nfx\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "import re \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e08c713",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21175b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training labels: [0 3 2 5 4 1]\n",
      "Unique validation labels: [0 2 3 1 4 5]\n",
      "Unique test labels: [0 1 4 3 2 5]\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_val = pd.read_csv(\"data/validation.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Check unique Emotions in each dataset\n",
    "print(\"Unique training labels:\", df_train['label'].unique())\n",
    "print(\"Unique validation labels:\", df_val['label'].unique())\n",
    "print(\"Unique test labels:\", df_test['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aa266",
   "metadata": {},
   "source": [
    "## Mapping Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66389f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "for df in [df_train, df_val, df_test]:\n",
    "    df['emotion'] = df['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a1b7e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in training set: text       0\n",
      "label      0\n",
      "emotion    0\n",
      "dtype: int64\n",
      "\n",
      "Training set emotion distribution:\n",
      " emotion\n",
      "joy         5362\n",
      "sadness     4666\n",
      "anger       2159\n",
      "fear        1937\n",
      "love        1304\n",
      "surprise     572\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Checking for Missing values in training data\n",
    "print(\"Missing values in training set:\", df_train.isnull().sum())\n",
    "\n",
    "#Value counts of each Emotions\n",
    "print(\"\\nTraining set emotion distribution:\\n\", df_train['emotion'].value_counts())\n",
    "\n",
    "dir(nfx)\n",
    "df_train['Clean_Text'] = df_train['text'].apply(nfx.remove_userhandles)\n",
    "df_train['Clean_Text'] = df_train['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df_test['Clean_Text'] = df_test['text'].apply(nfx.remove_userhandles)\n",
    "df_test['Clean_Text'] = df_test['Clean_Text'].apply(nfx.remove_stopwords)\n",
    "\n",
    "df_val['Clean_Text'] = df_val['text'].apply(nfx.remove_userhandles)\n",
    "df_val['Clean_Text'] = df_val['Clean_Text'].apply(nfx.remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fce924",
   "metadata": {},
   "source": [
    "## Text Preprocessing with Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b07737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "\n",
    "# # Define a function for tokenization and stemming\n",
    "# def stemmed_tokenizer(text):\n",
    "#     tokens = nltk.word_tokenize(text)\n",
    "#     return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get the Part of Speech (POS) for accurate lemmatization\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if POS not found\n",
    "\n",
    "# Define a function for tokenization and lemmatization\n",
    "def lemmatized_tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016330d7",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d61cac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\udit0\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the training data\n",
    "# vectorizer = CountVectorizer(tokenizer=lemmatized_tokenizer)\n",
    "\n",
    "# Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatized_tokenizer, max_features=5000)\n",
    "\n",
    "tdm_train = vectorizer.fit_transform(df_train['Clean_Text'])\n",
    "tdm_val = vectorizer.transform(df_val['Clean_Text'])\n",
    "tdm_test = vectorizer.transform(df_test['Clean_Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730c780",
   "metadata": {},
   "source": [
    "## Training the Logistic Regression Model - OneVsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a1a5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (OvR) Validation Accuracy: 0.8645\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "log_model_ovr = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "log_model_ovr.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the model\n",
    "y_val_pred_log_ovr = log_model_ovr.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_log_ovr = accuracy_score(df_val['label'], y_val_pred_log_ovr)\n",
    "print(\"Logistic Regression (OvR) Validation Accuracy:\", val_accuracy_log_ovr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33fec8",
   "metadata": {},
   "source": [
    "## Training the Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d1cd455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Accuracy: 0.876\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Random Forest model\n",
    "y_val_pred_rf = rf_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_rf = accuracy_score(df_val['label'], y_val_pred_rf)\n",
    "print(\"Random Forest Validation Accuracy:\", val_accuracy_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767b2c",
   "metadata": {},
   "source": [
    "## Training the SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "790b3817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated LinearSVC Validation Accuracy: 0.8855\n"
     ]
    }
   ],
   "source": [
    "# #Train the SVM model\n",
    "# svm_model = SVC(probability=True,random_state=42)\n",
    "# svm_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# # Validate the SVM model\n",
    "# y_val_pred_svm = svm_model.predict(tdm_val)\n",
    "\n",
    "# # Calculate and print validation accuracy\n",
    "# val_accuracy_svm = accuracy_score(df_val['label'], y_val_pred_svm)\n",
    "# print(\"SVM Validation Accuracy:\", val_accuracy_svm)\n",
    "\n",
    "base_svm_model = LinearSVC(random_state=42)\n",
    "svm_model = CalibratedClassifierCV(base_svm_model)\n",
    "svm_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the calibrated model\n",
    "y_val_pred_svm = svm_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_svm = accuracy_score(df_val['label'], y_val_pred_svm)\n",
    "print(\"Calibrated LinearSVC Validation Accuracy:\", val_accuracy_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711025f2",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd732e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Validation Accuracy: 0.752\n"
     ]
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Naive Bayes model\n",
    "y_val_pred_nb = nb_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_nb = accuracy_score(df_val['label'], y_val_pred_nb)\n",
    "print(\"Naive Bayes Validation Accuracy:\", val_accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79555730",
   "metadata": {},
   "source": [
    "## Training the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12d91ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation Accuracy: 0.838\n"
     ]
    }
   ],
   "source": [
    "# Train a Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the Decision Tree model\n",
    "y_val_pred_dt = dt_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_dt = accuracy_score(df_val['label'], y_val_pred_dt)\n",
    "print(\"Decision Tree Validation Accuracy:\", val_accuracy_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386818a",
   "metadata": {},
   "source": [
    "## Training the XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15071332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation Accuracy: 0.8795\n"
     ]
    }
   ],
   "source": [
    "# Train a XGBoost model\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "xgb_model.fit(tdm_train, df_train['label'])\n",
    "\n",
    "# Validate the XGBoost model\n",
    "y_val_pred_xgb = xgb_model.predict(tdm_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy_xgb = accuracy_score(df_val['label'], y_val_pred_xgb)\n",
    "print(\"XGBoost Validation Accuracy:\", val_accuracy_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e1ac7",
   "metadata": {},
   "source": [
    "## Comparing Models and Testing the Best combination of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f9752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model or Combination:\n",
      "SVM\n",
      "XGBoost\n",
      "Best Validation Accuracy: 0.892\n",
      "Test Accuracy of Best Model or Combination: 0.8915\n"
     ]
    }
   ],
   "source": [
    "model_dict = {\n",
    "    \"Logistic Regression\": log_model_ovr,\n",
    "    \"Random Forest\": rf_model,\n",
    "    \"SVM\": svm_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"Naive Bayes\": nb_model,\n",
    "    \"Decision Tree\": dt_model,\n",
    "}\n",
    "\n",
    "best_combination = None\n",
    "best_model_or_combination = None\n",
    "best_accuracy = 0\n",
    "\n",
    "# Test single models\n",
    "for name, model in model_dict.items():\n",
    "    model.fit(tdm_train, df_train['label'])  # Train on training set\n",
    "    y_val_pred = model.predict(tdm_val)  # Validate on validation set\n",
    "    val_accuracy = accuracy_score(df_val['label'], y_val_pred)\n",
    "\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_combination = [(name, model)]\n",
    "        best_model_or_combination = model\n",
    "        best_accuracy = val_accuracy\n",
    "\n",
    "# Test all combinations of models (size 2 to all models)\n",
    "for r in range(2, len(model_dict) + 1):  # Combinations of size 2 to all models\n",
    "    for subset in combinations(model_dict.items(), r):\n",
    "        model_subset = [(name, model) for name, model in subset]\n",
    "        combined_model = VotingClassifier(estimators=model_subset, voting='soft', n_jobs=-1)\n",
    "\n",
    "        combined_model.fit(tdm_train, df_train['label'])  # Train combined model\n",
    "        y_val_pred_combined = combined_model.predict(tdm_val)  # Validate on validation set\n",
    "        val_accuracy_combined = accuracy_score(df_val['label'], y_val_pred_combined)\n",
    "\n",
    "        if val_accuracy_combined > best_accuracy:\n",
    "            best_combination = model_subset\n",
    "            best_model_or_combination = combined_model\n",
    "            best_accuracy = val_accuracy_combined\n",
    "\n",
    "# Print the best model or combination and its accuracy\n",
    "print(\"Best Model or Combination:\")\n",
    "for name, _ in best_combination:\n",
    "    print(name)\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Test the best model or combination\n",
    "y_test_pred = best_model_or_combination.predict(tdm_test)\n",
    "test_accuracy = accuracy_score(df_test['label'], y_test_pred)\n",
    "print(f\"Test Accuracy of Best Model or Combination: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b5021",
   "metadata": {},
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "364b4389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report for the Test Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93       581\n",
      "           1       0.91      0.92      0.92       695\n",
      "           2       0.77      0.74      0.75       159\n",
      "           3       0.88      0.89      0.89       275\n",
      "           4       0.87      0.88      0.87       224\n",
      "           5       0.72      0.74      0.73        66\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.85      0.85      0.85      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n",
      "\n",
      "Confusion Matrix for the Test Dataset:\n",
      "[[537  14   2  16   9   3]\n",
      " [  8 638  31   5   6   7]\n",
      " [  6  31 117   2   1   2]\n",
      " [ 13   8   1 246   6   1]\n",
      " [ 13   0   0   9 196   6]\n",
      " [  2   7   0   0   8  49]]\n"
     ]
    }
   ],
   "source": [
    "# Print classification report and confusion matrix for the test dataset\n",
    "print(\"\\nClassification Report for the Test Dataset:\")\n",
    "print(classification_report(df_test['label'], y_test_pred))\n",
    "print(\"\\nConfusion Matrix for the Test Dataset:\")\n",
    "print(confusion_matrix(df_test['label'], y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ee54c",
   "metadata": {},
   "source": [
    "## Adding Predictions to the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a68d6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predicted emotions to the test dataframe\n",
    "df_test['predicted_emotion'] = [\n",
    "    label_mapping[label] if label in label_mapping else 'unknown'\n",
    "    for label in y_test_pred\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466d394",
   "metadata": {},
   "source": [
    "## Displaying Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7560c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Dataset Predictions:\n",
      "                                                text  emotion  \\\n",
      "0  im feeling rather rotten so im not very ambiti...  sadness   \n",
      "1          im updating my blog because i feel shitty  sadness   \n",
      "2  i never make her separate from me because i do...  sadness   \n",
      "3  i left with my bouquet of red and yellow tulip...      joy   \n",
      "4    i was feeling a little vain when i did this one  sadness   \n",
      "\n",
      "  predicted_emotion  \n",
      "0           sadness  \n",
      "1           sadness  \n",
      "2           sadness  \n",
      "3               joy  \n",
      "4           sadness  \n"
     ]
    }
   ],
   "source": [
    "# Display the predictions in the test dataset\n",
    "print(\"\\nTest Dataset Predictions:\")\n",
    "print(df_test[['text', 'emotion', 'predicted_emotion']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b00f3",
   "metadata": {},
   "source": [
    "## Predicting Emotion Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b57913b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(sentence, model, vectorizer, label_mapping):\n",
    "    sentence_tdm = vectorizer.transform([sentence])\n",
    "    probabilities = model.predict_proba(sentence_tdm)[0]\n",
    "    emotions = [label_mapping[i] for i in range(len(probabilities))]\n",
    "    \n",
    "    # Plot the predicted probabilities\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=emotions, y=probabilities)\n",
    "    plt.title('Emotion Prediction')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c899a5",
   "metadata": {},
   "source": [
    "## Using the Predict Emotion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafe36ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predict emotion for a specific sentence\n",
    "# predict_emotion(\"I heard strange noises outside last night, and I couldn't sleep at all. My heart was racing, and I kept imagining all sorts of dangers lurking in the dark.\", best_model, vectorizer, label_mapping)\n",
    "\n",
    "# Function for user input prediction\n",
    "def user_input_prediction(model, vectorizer, label_mapping):\n",
    "    try:\n",
    "        sentence = input(\"Please enter a sentence to predict emotion: \")\n",
    "        predict_emotion(sentence, model, vectorizer, label_mapping)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Call the user input function\n",
    "user_input_prediction(best_model_or_combination, vectorizer, label_mapping)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
